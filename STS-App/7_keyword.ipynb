{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords extracted and saved to text files!\n",
      "Keywords extracted and saved to text files!\n",
      "Keywords extracted and saved to text files!\n",
      "Keywords extracted and saved to text files!\n",
      "Keywords extracted and saved to text files!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m wrong_answered_question \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrong_answered_question\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Split the wrong answered questions into individual questions\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m questions \u001b[38;5;241m=\u001b[39m \u001b[43mwrong_answered_question\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Prepare data for the text file\u001b[39;00m\n\u001b[0;32m     56\u001b[0m keyword_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Function to extract keywords from a sentence\n",
    "def extract_keywords(sentence):\n",
    "    # Regular expression to find phrases inside single quotes\n",
    "    quoted_phrases = re.findall(r\"'([^']+)'\", sentence)\n",
    "    \n",
    "    # Remove the quoted phrases from the sentence\n",
    "    cleaned_sentence = re.sub(r\"'[^']+'\", '', sentence)\n",
    "    \n",
    "    # Tokenize the cleaned sentence and the quoted phrases\n",
    "    tokens = word_tokenize(cleaned_sentence)\n",
    "    for phrase in quoted_phrases:\n",
    "        tokens.append(phrase)\n",
    "\n",
    "    # Part-of-speech tagging\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Extract keywords, excluding words within single quotes\n",
    "    keywords = [word for word, pos in tagged_tokens if (pos.startswith('NN') or pos.startswith('JJ')) and \"'\" not in word]\n",
    "    return keywords\n",
    "\n",
    "# Function to clean the question text by removing question numbers\n",
    "def clean_question_text(question):\n",
    "    # Remove patterns like 'q1)', 'q2)', etc.\n",
    "    cleaned_question = re.sub(r'q\\d+\\)', '', question).strip()\n",
    "    return cleaned_question\n",
    "\n",
    "# Create the directory for the keyword files if it doesn't exist\n",
    "if not os.path.exists('./keyword'):\n",
    "    os.makedirs('./keyword')\n",
    "\n",
    "# Infinite loop to run the script repeatedly\n",
    "while True:\n",
    "    # Load the CSV file\n",
    "    with open('merged_output_with_full_wrong_answers.csv', 'r') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        \n",
    "        for row in reader:\n",
    "            rollno = row['rollno']\n",
    "            wrong_answered_question = row['wrong_answered_question']\n",
    "            \n",
    "            # Split the wrong answered questions into individual questions\n",
    "            questions = wrong_answered_question.split('\\n\\n')\n",
    "            \n",
    "            # Prepare data for the text file\n",
    "            keyword_str = ''\n",
    "            for question in questions:\n",
    "                # Clean the question text\n",
    "                cleaned_question = clean_question_text(question)\n",
    "                \n",
    "                # Extract keywords from the cleaned question text\n",
    "                keywords = extract_keywords(cleaned_question)\n",
    "                keyword_str += '\\n'.join(keywords) + '\\n'  # Separate keywords with newlines\n",
    "            \n",
    "            # Save the keywords to a text file\n",
    "            with open(f'./keyword/{rollno}.txt', 'w') as outfile:\n",
    "                outfile.write(keyword_str.strip())\n",
    "    \n",
    "    print(\"Keywords extracted and saved to text files!\")\n",
    "    \n",
    "    # Wait for 10 seconds before the next iteration\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
